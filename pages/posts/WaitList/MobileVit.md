---
title: MobileViT的分析与实现
author: OceanPresent
time: '2022-12-23'
lang: zh-CN
---

[[toc]]

# 网络背景

## ViT的出现

为了解决在NLP领域中的常用网络模型*RNN*，*LSTM*，*GRU*等由于无法并行化计算而导致模型训练速度和计算速度慢的问题，Google团队提出了*Transformer* 模型，并在后续实验中逐渐用于图像分类，演化出*vision Transformer(ViT)*。

## CNN模型的改进

*CNN*在*AlexNet*被提出并使用在图像分类后，便广泛的应用于计算机视觉分类的任务，近年来随着网络准确率的提升，网络结构愈发复杂，网络参数急剧增长，为了在保持精度的同时减少模型的参数，加快网络速度，Google提出了*MobileNet(v1,v2)*轻量化网络，在训练资源受限的移动端和边缘计算上可以进行良好的适配。

## 融合型网络兴起

随着各种网络结构的提出，为了使用自我注意机制捕获远程依赖关系，并使用 *CNN* 中的本地内核捕获本地信息，以提高视觉任务的性能，融合型网络在计算机视觉领域被人们首次提出，取得了非凡的效果。

# Vision Transformer 原理

## ViT是什么

随着基于自注意（*Self-Attention*）结构的模型的发展，特别是Transformer模型的提出，极大地促进了自然语言处理模型的发展。由于Transformers的计算效率和可扩展性，它已经能够训练具有超过100B参数的空前规模的模型。
ViT则是自然语言处理和计算机视觉两个领域的融合结晶。在不依赖卷积操作的情况下，依然可以在图像分类任务上达到很好的效果。

## VIT的影响

2021年在计算机视觉领域影响力最大的工作
1. 它挑战了自从2012年AlexNet提出以来卷积神经网络在计算机视觉里绝对统治的地位
2. 如果在足够多的数据上做预训练，也可以不需要卷积神经网路，直接使用标准的transformer也能够把视觉问题解决的很好
3. 它打破了CV和NLP在模型上的壁垒，开启了CV的一个新时代。

## 回顾Vit

这个图讲的是一个seq2seq的model，左侧为 *Encoder Block*，右侧为 *Decoder Block*。黄色圈中的部分为**Multi-Head Attention**，是由多个*Self-Attention*组成的，*Encoder block* 包含一个 *Multi-Head Attention*，而 *Decoder block* 包含两个 *Multi-Head Attention* (其中有一个用到 Masked)。

*Multi-Head Attention* 上方还包括一个 Add & Norm 层，Add 表示残差连接 *(Residual Connection)* 用于防止网络退化，Norm 表示 *Layer Normalization*，用于对每一层的激活值进行归一化。

transformer中最主要的操作就是自注意力操作，自注意力操作就是每个元素都要跟每个元素进行互动，两两互相的，然后算得一个attention，用这个自注意力的图去做加权平均，最后得到输出因为在做自注意力的时候是两两互相的，这个计算复杂度是跟序列的长度呈平方倍的。

目前一般在自然语言处理中，硬件能支持的序列长度一般也就是几百或者是上千（比如说BERT的序列长度也就是512）

## 将Transformer运用到视觉领域的难处

首先要解决的是如何把一个2D的图片变成一个1D的序列（或者说变成一个集合）。最直观的方式就是把每个像素点当成元素，将图片拉直放进transformer里，看起来比较简单，但是实现起来复杂度较高。

所以很多工作就是在研究如何将自注意力用到机器视觉中：一些工作把卷积神经网络和自注意力混到一起用；另外一些工作就是整个将卷积神经网络换掉，全部用自注意力。这些方法其实都是在干一个事情：因为序列长度太长，所以导致没有办法将transformer用到视觉中，所以就想办法降低序列长度。
 
这种方式虽然理论上是非常高效的，但事实上因为自注意力操作都是一些比较特殊的操作，所以说无法在现在的硬件上进行加速，所以就导致很难训练出一个大模型，所以截止到目前为止，像孤立自注意力和轴自注意力的模型都还没有做到很大，跟百亿、千亿级别的大transformer模型比还是差的很远。

## Vit总览

```bash
ViT想要做的就是直接应用一个标准的transformer直接作用于图片，尽量做少的修改（不做任何针对视觉任务的特定改变），看看这样的transformer能不能在视觉领域中扩展得很大很好 。
```

如果直接使用transformer，就要解决序列长度的问题

1. vision transformer将一张图片打成了很多的patch，每一个patch是16*16
2. 假如图片的大小是224*224，则sequence lenth（序列长度）就是N=224*224=50176，如果换成patch，一个patch相当于一个元素的话，有效的长宽就变成了224/16=14，所以最后的序列长度就变成了N=14*14=196，所以现在图片就只有196个元素了，196对于普通的transformer来说是可以接受的。
3. 然后将每一个patch当作一个元素，通过一个fc layer（全连接层）就会得到一个linear embedding，这些就会当作输入传给transformer，这时候一张图片就变成了一个一个的图片块了，可以将这些图片块当成是NLP中的单词，一个句子中有多少单词就相当于是一张图片中有多少个patch，这就是为什么原论文说一张图片等价于很多16*16的单词。

```bash
模型指标：ViT模型在常用数据集上进行迁移学习，最终指标如图所示。可以看到，在ImageNet上，ViT达到的最高指标为88.55%；在ImageNet ReaL上，ViT达到的最高指标为90.72%；在CIFAR100上，ViT达到的最高指标为94.55%；在VTAB(19 tasks)上，ViT达到的最高指标为88.55%。
```

## Vit模型整体结构

## Vit模型特点

1. 作为CV领域最经典的 Transformer 算法之一，不同于传统的CNN算法，ViT尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。
2. 数据集的原图像被划分为多个patch后，将二维patch（不考虑channel）转换为一维向量，再加上类别向量与位置向量作为模型输入。
3. 模型主体的Block结构是基于Transformer的Encoder结构，但是调整了Normalization的位置，其中，最主要的结构依然是Multi-head Attention结构。
4. 模型在Blocks堆叠后接全连接层，接受类别向量的输出作为输入并用于分类。通常情况下，我们将最后的全连接层称为Head，Transformer Encoder部分为backbone。

# MobileViTv1 原理

## 提出背景

轻量级的卷积神经网络在移动视觉任务中非常有用，它们的空间归纳偏置允许它们在不同的视觉任务中以较少的参数学习表征。然而，这些网络在空间上是局部建模的。如果想要学习全局表征，可以采用基于自注意的视觉Transformer(ViT)。但是与CNN不同，ViT是的参数量比较大。作者结合CNN和ViTs的优势，为移动视觉任务构建一个轻量级、低延迟的网络。提出了MobileViT，一种用于移动设备的轻量级通用视觉Transformer。

Transformer弊端：
1. Transformer参数多，算力要求高
2. Transformer迁移到其他任务比较繁琐
3. Transformer缺少空间归纳偏置
4. Transformer模型训练困难

## 模型结果

实验结果表明，MobileViT在不同的任务和数据集上显著优于基于CNN和ViT的网络。在ImageNet-1k数据集上，MobileViT实现了78.4%的Top-1精度，拥有约600万个参数，对于类似数量的参数，其精度分别比MobileNetv3（基于CNN）和DeIT（基于ViT）高3.2%和6.2%。在MS-COCO目标检测任务中，对于相同数量的参数，MobileViT比MobileNetv3的准确度高5.7%。

## 模型整体结构

模型重点结构：MV2(inverted residual block)、MobileViT block


## MV2 Block

结构：与传统的Residual Block不同，其通过翻转升维和降维顺序，先对特征进行细化扩充，再通过深度可分离卷积提取特征后进行降维，减少运算。


## MobileViT block 

结构：通过将输入数据进行本地表征操作后送入全局表征操作，提取全局特征最后将输入数据与全局表征相加，通过卷积层进行特征融合。

# 实践一：MobileViTv1的实现

## 概述

1. 使用训练集图片大小为256*256，样本量: 1309，验证集样本量: 328，共10个分类。
2. 使用SA2注意力数据集对模型进行测试。经过20次迭代后，在训练集上准确率为92.41%，验证集准确率为83.06%
3. 使用MobileNetv2模型进行对比试验。在训练集上准确率为87.9%，验证集准确率为73.7%


# 实践二：用MobileViTv1进行垃圾分类

## 概述

1. 使用复现出的模型进行垃圾分类实践。使用训练集图片大小为256*256，样本量: 127，验证集样本量: 16，共6个分类。
2. 使用垃圾分类数据集对模型进行测试。经过50次迭代后，在训练集上准确率为96.05%，验证集准确率为82.94%
